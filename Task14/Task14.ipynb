{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv files to data frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv',encoding = \"latin1\")\n",
    "test_df = pd.read_csv('data/test.csv',encoding = \"latin1\")\n",
    "attributes_df = pd.read_csv('data/attributes.csv',encoding = \"latin1\")\n",
    "product_descriptions_df = pd.read_csv('data/product_descriptions.csv',encoding = \"latin1\")\n",
    "sample_submission_df = pd.read_csv('data/sample_submission.csv', encoding = \"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 int64\n",
      "product_uid        int64\n",
      "product_title     object\n",
      "search_term       object\n",
      "relevance        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_uid             int64\n",
      "product_description    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_uid             int64\n",
      "product_description    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_uid                                product_description\n",
      "0       100001  Not only do angles make joints stronger, they ...\n",
      "1       100002  BEHR Premium Textured DECKOVER is an innovativ...\n",
      "2       100003  Classic architecture meets contemporary design...\n",
      "3       100004  The Grape Solar 265-Watt Polycrystalline PV So...\n",
      "4       100005  Update your bathroom with the Delta Vero Singl...\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK's stopwords list and WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and tokenize the raw text by:\n",
    "    1. Convert to lower case\n",
    "    2. Remove apostrophe\n",
    "    3. Remove Punctuation\n",
    "    4. Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "   \n",
    "    # convert text to lower case\n",
    "    lowercase = str(text).lower()\n",
    "    \n",
    "    #remove 's from string\n",
    "    apoRemoved = lowercase.replace(\"'s\",\"\")\n",
    "    \n",
    "    #convert don't to dont\n",
    "    apoRemoved = apoRemoved.replace(\"'\",\"\")\n",
    "    \n",
    "    #handle other punctuations\n",
    "    transtable = str.maketrans(string.punctuation,\"                                \")\n",
    "    brokenWords = apoRemoved.translate(transtable)\n",
    "    \n",
    "    #convert string to list of words\n",
    "    listOfWords =  nltk.word_tokenize(brokenWords)\n",
    "    \n",
    "    #lemmatize text\n",
    "    lemmatizedList=[lemmatizer.lemmatize(word) for word in listOfWords]\n",
    "   \n",
    "    return lemmatizedList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'here', 'dont', 'car']\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "text = \"Dogs Here's don't cars.\"\n",
    "print(process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train_df\n",
    "def process_train_df(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \n",
    "    newdf=df\n",
    "    for i,row in newdf.iterrows():\n",
    "#         if i>5:\n",
    "#             break\n",
    "#         print(\"raw: \",text)\n",
    "        newdf.at[i,'product_title'] = process(row['product_title'])\n",
    "#         print(\"processed: \",df.iloc[i]['product_title'])\n",
    "    return newdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_uid                                      product_title  \\\n",
      "0   2       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "1   3       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "2   9       100002  [behr, premium, textured, deckover, 1, gal, sc...   \n",
      "3  16       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "4  17       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "\n",
      "          search_term  relevance  \n",
      "0       angle bracket       3.00  \n",
      "1           l bracket       2.50  \n",
      "2           deck over       3.00  \n",
      "3    rain shower head       2.33  \n",
      "4  shower only faucet       2.67  \n"
     ]
    }
   ],
   "source": [
    "processed_train_df = process_train_df(train_df)\n",
    "lentr = len(processed_train_df['product_uid'])\n",
    "print(processed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74067\n"
     ]
    }
   ],
   "source": [
    "print(lentr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process \n",
    "def process_product_descriptions_df(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \n",
    "    newdf=df\n",
    "    for i,row in newdf.iterrows():\n",
    "#         if i>5:\n",
    "#             break\n",
    "#         print(\"raw: \",text)\n",
    "        newdf.at[i,'product_description'] = process(row['product_description'])\n",
    "#         print(\"processed: \",df.iloc[i]['product_title'])\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_uid                                product_description\n",
      "0       100001  [not, only, do, angle, make, joint, stronger, ...\n",
      "1       100002  [behr, premium, textured, deckover, is, an, in...\n",
      "2       100003  [classic, architecture, meet, contemporary, de...\n",
      "3       100004  [the, grape, solar, 265, watt, polycrystalline...\n",
      "4       100005  [update, your, bathroom, with, the, delta, ver...\n"
     ]
    }
   ],
   "source": [
    "processed_product_descriptions_df = process_product_descriptions_df(product_descriptions_df)\n",
    "print(processed_product_descriptions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [not, only, do, angle, make, joint, stronger, ...\n",
      "1    [behr, premium, textured, deckover, is, an, in...\n",
      "2    [classic, architecture, meet, contemporary, de...\n",
      "3    [the, grape, solar, 265, watt, polycrystalline...\n",
      "4    [update, your, bathroom, with, the, delta, ver...\n",
      "Name: product_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_product_descriptions_df[\"product_description\"].head(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(processed_product_descriptions_df[\"product_description\"].head(5) )\n",
    "# from nltk.corpus import wordnet\n",
    "# import numpy as np\n",
    "# query = 'shower head'\n",
    "# qa= query.split(\" \")\n",
    "# c=0\n",
    "# w1 = wordnet.synsets(\"dog\")[0]\n",
    "# w2 = wordnet.synsets(\"animal\")[0]\n",
    "# print(w1.wup_similarity(w2))\n",
    "# sLength = len(processed_product_descriptions_df[\"product_description\"])\n",
    "# print(sLength)\n",
    "# e = pd.Series(np.random.randn(sLength))\n",
    "# processed_product_descriptions_df = processed_product_descriptions_df.assign(e=e.values)\n",
    "# print(processed_product_descriptions_df.head())\n",
    "# print('-----------------------------------------')\n",
    "# total_sim=0\n",
    "# word_count=0\n",
    "# for index,rr in processed_product_descriptions_df.iterrows():\n",
    "#     #print(lin)\n",
    "#     lin = rr[\"product_description\"]\n",
    "#     query=rr[\"\"]\n",
    "#     query=rr[]\n",
    "    \n",
    "#     for word1 in lin:\n",
    "#         w1 = wordnet.synsets(word1)\n",
    "#         for word2 in qa:\n",
    "#             w2 = wordnet.synsets(word2)\n",
    "#             if w1 and w2:\n",
    "#                 sim = w1[0].wup_similarity(w2[0])\n",
    "#                 #print(sim)\n",
    "#                 if sim!=None and sim > .1:\n",
    "#                     total_sim=total_sim+sim\n",
    "#                     word_count=word_count+1\n",
    "#     if word_count==0:\n",
    "#          rr['e']=None\n",
    "#     else:\n",
    "#         rr['e'] = total_sim/word_count\n",
    "#     print(rr['e'])\n",
    "        \n",
    "        \n",
    "                \n",
    "#     c=c+1\n",
    "#     if c> 4:\n",
    "#         break\n",
    "# print(c)\n",
    "    #if c> 4:\n",
    "        #break\n",
    "# query = \"shower head\"\n",
    "# from lsa.search.machine import SearchMachine\n",
    "# sm = SearchMachine(latent_dimensions=150, index_backend='lsa.keeper.backends.JsonIndexBackend',\n",
    "#                    keep_index_info={'path_to_index_folder': 'index'},\n",
    "#                    db_backend='lsa.db.mysql.MySQLBackend',\n",
    "#                    db_credentials={'db_name': 'news', 'user': 'root', 'password': 'one2012gtr'},\n",
    "#                    tables_info={\n",
    "#                        'news_news': {'fields': ('title', 'text'), 'pk_field_name': 'id', 'prefix': '', 'where': 'id < 300'}\n",
    "#                    },\n",
    "#                    decimals=3,\n",
    "#                    use_tf_idf=False\n",
    "#                    )\n",
    "\n",
    "# sm.build_index()\n",
    "# res = sm.search('natural language query', with_distances=True, limit=10)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>[not, only, do, angle, make, joint, stronger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>[behr, premium, textured, deckover, is, an, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>[classic, architecture, meet, contemporary, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>[the, grape, solar, 265, watt, polycrystalline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>[update, your, bathroom, with, the, delta, ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                product_description\n",
       "0       100001  [not, only, do, angle, make, joint, stronger, ...\n",
       "1       100002  [behr, premium, textured, deckover, is, an, in...\n",
       "2       100003  [classic, architecture, meet, contemporary, de...\n",
       "3       100004  [the, grape, solar, 265, watt, polycrystalline...\n",
       "4       100005  [update, your, bathroom, with, the, delta, ver..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_product_descriptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(processed_train_df,processed_product_descriptions_df, on='product_uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>[simpson, strong, tie, 12, gauge, angle]</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.00</td>\n",
       "      <td>[not, only, do, angle, make, joint, stronger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>[simpson, strong, tie, 12, gauge, angle]</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.50</td>\n",
       "      <td>[not, only, do, angle, make, joint, stronger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>[behr, premium, textured, deckover, 1, gal, sc...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.00</td>\n",
       "      <td>[behr, premium, textured, deckover, is, an, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>[delta, vero, 1, handle, shower, only, faucet,...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>2.33</td>\n",
       "      <td>[update, your, bathroom, with, the, delta, ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>100005</td>\n",
       "      <td>[delta, vero, 1, handle, shower, only, faucet,...</td>\n",
       "      <td>shower only faucet</td>\n",
       "      <td>2.67</td>\n",
       "      <td>[update, your, bathroom, with, the, delta, ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
       "1   3       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
       "2   9       100002  [behr, premium, textured, deckover, 1, gal, sc...   \n",
       "3  16       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
       "4  17       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
       "\n",
       "          search_term  relevance  \\\n",
       "0       angle bracket       3.00   \n",
       "1           l bracket       2.50   \n",
       "2           deck over       3.00   \n",
       "3    rain shower head       2.33   \n",
       "4  shower only faucet       2.67   \n",
       "\n",
       "                                 product_description  \n",
       "0  [not, only, do, angle, make, joint, stronger, ...  \n",
       "1  [not, only, do, angle, make, joint, stronger, ...  \n",
       "2  [behr, premium, textured, deckover, is, an, in...  \n",
       "3  [update, your, bathroom, with, the, delta, ver...  \n",
       "4  [update, your, bathroom, with, the, delta, ver...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [not, only, do, angle, make, joint, stronger, ...\n",
      "1    [not, only, do, angle, make, joint, stronger, ...\n",
      "2    [behr, premium, textured, deckover, is, an, in...\n",
      "3    [update, your, bathroom, with, the, delta, ver...\n",
      "4    [update, your, bathroom, with, the, delta, ver...\n",
      "Name: product_description, dtype: object\n",
      "0.875\n",
      "74067\n",
      "   id  product_uid                                      product_title  \\\n",
      "0   2       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "1   3       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "2   9       100002  [behr, premium, textured, deckover, 1, gal, sc...   \n",
      "3  16       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "4  17       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "\n",
      "          search_term  relevance  \\\n",
      "0       angle bracket       3.00   \n",
      "1           l bracket       2.50   \n",
      "2           deck over       3.00   \n",
      "3    rain shower head       2.33   \n",
      "4  shower only faucet       2.67   \n",
      "\n",
      "                                 product_description         e        e2  \\\n",
      "0  [not, only, do, angle, make, joint, stronger, ...  0.982371  0.587110   \n",
      "1  [not, only, do, angle, make, joint, stronger, ...  0.531522  0.059904   \n",
      "2  [behr, premium, textured, deckover, is, an, in... -0.408685  1.037178   \n",
      "3  [update, your, bathroom, with, the, delta, ver...  0.001305  0.116359   \n",
      "4  [update, your, bathroom, with, the, delta, ver... -0.121866  0.975074   \n",
      "\n",
      "         e3        e4  \n",
      "0 -0.392441  0.240527  \n",
      "1  1.346990  0.130940  \n",
      "2  0.673378  1.811398  \n",
      "3  0.376543  0.949711  \n",
      "4 -0.540621 -1.202598  \n",
      "-----------------------------------------\n",
      "0.701032301032301\n",
      "0.6838842975206613\n",
      "0.6840867620885119\n",
      "0.6681900830507641\n",
      "0.683797729618163\n",
      "0.8857808857808859\n",
      "0.7296000417710946\n",
      "0.8499791144527986\n",
      "0.8276847421584264\n",
      "0\n",
      "0.8933333333333333\n",
      "0.6673273597938204\n",
      "0.6861111111111111\n",
      "0.6600968652568242\n",
      "0.748723373543684\n",
      "0\n",
      "0.8537796697626421\n",
      "0.645375694137304\n",
      "0.6658877586122167\n",
      "0.6221937867964699\n",
      "0.6870525988173047\n",
      "0.6487917096663226\n",
      "0.6864465084279324\n",
      "0.6864465084279324\n",
      "0\n",
      "0.6466846510841952\n",
      "0.6645166838665291\n",
      "0.7474872564655844\n",
      "0.6630890952872377\n",
      "0.6501284182140976\n",
      "0.6983111715612728\n",
      "0.674243639664328\n",
      "0.7079282430675623\n",
      "0.7727109593837533\n",
      "1.0\n",
      "0.6879171754171755\n",
      "0.6986772486772486\n",
      "0.8888888888888888\n",
      "0.6955459546959075\n",
      "0.7236553275156218\n",
      "0.6736842105263159\n",
      "0.7374868499435974\n",
      "0.7374868499435974\n",
      "0.698119000133881\n",
      "0.6928571428571428\n",
      "0.8259803921568627\n",
      "0.6259259259259259\n",
      "0.6910256410256409\n",
      "0.664435598800924\n",
      "0.6570149890099585\n",
      "0.65794899495605\n",
      "0.6267213683719105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0f86f03186a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlin2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"product_description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0msemanticMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlin2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msemanticMatch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;31m#print( str(rr['e'])+'\\t'+str(rr['e2'])+'\\t' +'\\t'+str(rr['relevance']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m#     for word1 in lin:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-0f86f03186a4>\u001b[0m in \u001b[0;36msemanticMatch2\u001b[0;34m(query, text)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;31m#print(sim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mwup_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    899\u001b[0m         subsumers = self.lowest_common_hypernyms(\n\u001b[1;32m    900\u001b[0m             \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0msimulate_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimulate_root\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mneed_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_min_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         )\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mlowest_common_hypernyms\u001b[0;34m(self, other, simulate_root, use_min_depth)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 unsorted_lch = [\n\u001b[0;32m--> 642\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m                 ]\n\u001b[1;32m    644\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 unsorted_lch = [\n\u001b[0;32m--> 642\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m                 ]\n\u001b[1;32m    644\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mmin_depth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mmin_depth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_min_depth\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0mhypernyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36minstance_hypernyms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minstance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_instance_hypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_related\u001b[0;34m(self, relation_symbol, sort)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_related\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation_symbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m         \u001b[0mget_synset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wordnet_corpus_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset_from_pos_and_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0mpointer_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pointers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelation_symbol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpointer_tuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(df_merge[\"product_description\"].head(5) )\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "query = 'shower head'\n",
    "qa= query.split(\" \")\n",
    "c=0\n",
    "w1 = wordnet.synsets(\"dog\")[0]\n",
    "w2 = wordnet.synsets(\"animal\")[0]\n",
    "print(w1.wup_similarity(w2))\n",
    "sLength = len(df_merge[\"product_description\"])\n",
    "print(sLength)\n",
    "e = pd.Series(np.random.randn(sLength))\n",
    "df_merge = df_merge.assign(e=e.values)\n",
    "e2 = pd.Series(np.random.randn(sLength))\n",
    "df_merge = df_merge.assign(e2=e2.values)\n",
    "e3 = pd.Series(np.random.randn(sLength))\n",
    "df_merge = df_merge.assign(e3=e3.values)\n",
    "e4 = pd.Series(np.random.randn(sLength))\n",
    "df_merge = df_merge.assign(e4=e4.values)\n",
    "print(df_merge.head())\n",
    "print('-----------------------------------------')\n",
    "total_sim=0\n",
    "word_count=0\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def semanticMatch2(query, text):\n",
    "    #print('-----------------------------------------')\n",
    "   \n",
    "    threshold2=58\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(lin)\n",
    "    total_sim=0\n",
    "    total_sim2=0\n",
    "    word_count=0\n",
    "    word_count2=0\n",
    "  \n",
    "    lin = text\n",
    "\n",
    "    qa= query.split(\" \")\n",
    "    lin2= text\n",
    "    total_sim=0\n",
    "    total_sim2=0\n",
    "    word_count=0\n",
    "    word_count=0\n",
    "    word_count2=0\n",
    "    for word1 in lin2:\n",
    "        w1 = wordnet.synsets(word1)\n",
    "        for word2 in qa:\n",
    "#             sim2=fuzz.ratio(word1, word2)\n",
    "#             if sim2!=None and sim2 > threshold2:\n",
    "#                     total_sim2=total_sim2+sim2\n",
    "#                     word_count2=word_count2+1\n",
    "            \n",
    "            w2 = wordnet.synsets(word2)\n",
    "            if w1 and w2:\n",
    "                sim = w1[0].wup_similarity(w2[0])\n",
    "                #print(sim)\n",
    "                if sim!=None and sim > .5:\n",
    "                    total_sim=total_sim+sim\n",
    "                    word_count=word_count+1\n",
    "    #word_count2=len(qa)\n",
    "   # print(word_count2)\n",
    "    #factor2= word_count2/(len(qa)*len(lin2) )\n",
    "    factor2 = word_count2\n",
    "    req=0\n",
    "    if word_count==0:\n",
    "         req=0\n",
    "    else:\n",
    "        req = total_sim/word_count\n",
    "    return(req)\n",
    "    \n",
    "\n",
    "\n",
    "def semanticMatch(query, title , desp):\n",
    "    #print('-----------------------------------------')\n",
    "   \n",
    "    threshold2=58\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(lin)\n",
    "    total_sim=0\n",
    "    total_sim2=0\n",
    "    word_count=0\n",
    "    word_count2=0\n",
    "  \n",
    "    lin = title\n",
    "\n",
    "    qa= query.split(\" \")\n",
    "    lin2= desp\n",
    "    for word1 in lin:\n",
    "        w1 = wordnet.synsets(word1)\n",
    "        for word2 in qa:\n",
    "#             sim2=fuzz.ratio(word1, word2)\n",
    "#             if sim2!=None and sim2 > threshold2:\n",
    "#                     total_sim2=total_sim2+sim2\n",
    "#                     word_count2=word_count2+1\n",
    "            \n",
    "            w2 = wordnet.synsets(word2)\n",
    "            \n",
    "            if w1 and w2:\n",
    "                sim = w1[0].wup_similarity(w2[0])\n",
    "                #print(sim)\n",
    "                if sim!=None and sim > .5:\n",
    "                    total_sim=total_sim+sim\n",
    "                    word_count=word_count+1\n",
    "    #word_count2=len(qa)\n",
    "    if word_count==0:\n",
    "         rr['e']=0\n",
    "    else:\n",
    "        rr['e'] = total_sim/word_count\n",
    "    \n",
    "    #print(rr['e'])\n",
    "#     if word_count2==0:\n",
    "#          rr['e3']=0\n",
    "#     else:\n",
    "#         #rr['e3'] = total_sim2/word_count2\n",
    "#         rr['e3'] = total_sim2/(word_count2)\n",
    "    #print(rr['e3'])\n",
    "    #print( str(rr['e'])+'\\t'+str(rr['e2'])+'\\t'+str(rr['e3'])+'\\t'+str(rr['e4']) )\n",
    "#     total_sim=0\n",
    "#     total_sim2=0\n",
    "#     word_count=0\n",
    "#     word_count=0\n",
    "#     word_count2=0\n",
    "#     for word1 in lin2:\n",
    "#         w1 = wordnet.synsets(word1)\n",
    "#         for word2 in qa:\n",
    "# #             sim2=fuzz.ratio(word1, word2)\n",
    "# #             if sim2!=None and sim2 > threshold2:\n",
    "# #                     total_sim2=total_sim2+sim2\n",
    "# #                     word_count2=word_count2+1\n",
    "            \n",
    "#             w2 = wordnet.synsets(word2)\n",
    "#             if w1 and w2:\n",
    "#                 sim = w1[0].wup_similarity(w2[0])\n",
    "#                 #print(sim)\n",
    "#                 if sim!=None and sim > .5:\n",
    "#                     total_sim=total_sim+sim\n",
    "#                     word_count=word_count+1\n",
    "#     #word_count2=len(qa)\n",
    "#    # print(word_count2)\n",
    "#     #factor2= word_count2/(len(qa)*len(lin2) )\n",
    "#     factor2 = word_count2\n",
    "#     if word_count==0:\n",
    "#          rr['e2']=0\n",
    "#     else:\n",
    "#         rr['e2'] = total_sim/word_count\n",
    "        \n",
    "#     if word_count2==0:\n",
    "#          rr['e4']=0\n",
    "#     else:\n",
    "\n",
    "        \n",
    "#         rr['e4'] = total_sim2/(factor2)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "for index,rr in df_merge.iterrows():\n",
    "    \n",
    "    \n",
    "  \n",
    "    lin = rr[\"product_title\"]\n",
    "    query=rr[\"search_term\"]\n",
    "    qa= query.split(\" \")\n",
    "    lin2= rr[\"product_description\"]\n",
    "    semanticMatch(query, lin , lin2)\n",
    "    print (semanticMatch2(query, lin2) )\n",
    "    #print( str(rr['e'])+'\\t'+str(rr['e2'])+'\\t' +'\\t'+str(rr['relevance'])) \n",
    "#     for word1 in lin:\n",
    "#         w1 = wordnet.synsets(word1)\n",
    "#         for word2 in qa:\n",
    "#             sim2=fuzz.ratio(word1, word2)\n",
    "#             if sim2!=None and sim2 > threshold2:\n",
    "#                     total_sim2=total_sim2+sim2\n",
    "#                     word_count2=word_count2+1\n",
    "            \n",
    "#             w2 = wordnet.synsets(word2)\n",
    "            \n",
    "#             if w1 and w2:\n",
    "#                 sim = w1[0].wup_similarity(w2[0])\n",
    "#                 #print(sim)\n",
    "#                 if sim!=None and sim > .5:\n",
    "#                     total_sim=total_sim+sim\n",
    "#                     word_count=word_count+1\n",
    "#     #word_count2=len(qa)\n",
    "#     if word_count==0:\n",
    "#          rr['e']=0\n",
    "#     else:\n",
    "#         rr['e'] = total_sim/word_count\n",
    "#     #print(rr['e'])\n",
    "#     if word_count2==0:\n",
    "#          rr['e3']=0\n",
    "#     else:\n",
    "#         #rr['e3'] = total_sim2/word_count2\n",
    "#         rr['e3'] = total_sim2/(word_count2)\n",
    "#     #print(rr['e3'])\n",
    "#     #print( str(rr['e'])+'\\t'+str(rr['e2'])+'\\t'+str(rr['e3'])+'\\t'+str(rr['e4']) )\n",
    "#     total_sim=0\n",
    "#     total_sim2=0\n",
    "#     word_count=0\n",
    "#     word_count=0\n",
    "#     word_count2=0\n",
    "#     for word1 in lin2:\n",
    "#         w1 = wordnet.synsets(word1)\n",
    "#         for word2 in qa:\n",
    "#             sim2=fuzz.ratio(word1, word2)\n",
    "#             if sim2!=None and sim2 > threshold2:\n",
    "#                     total_sim2=total_sim2+sim2\n",
    "#                     word_count2=word_count2+1\n",
    "            \n",
    "#             w2 = wordnet.synsets(word2)\n",
    "#             if w1 and w2:\n",
    "#                 sim = w1[0].wup_similarity(w2[0])\n",
    "#                 #print(sim)\n",
    "#                 if sim!=None and sim > .5:\n",
    "#                     total_sim=total_sim+sim\n",
    "#                     word_count=word_count+1\n",
    "#     #word_count2=len(qa)\n",
    "#     print(word_count2)\n",
    "#     #factor2= word_count2/(len(qa)*len(lin2) )\n",
    "#     factor2 = word_count2\n",
    "#     if word_count==0:\n",
    "#          rr['e2']=0\n",
    "#     else:\n",
    "#         rr['e2'] = total_sim/word_count\n",
    "        \n",
    "#     if word_count2==0:\n",
    "#          rr['e4']=0\n",
    "#     else:\n",
    "# #         print('----')\n",
    "        \n",
    "# #         print(total_sim2)\n",
    "# #         print(word_count2)\n",
    "# #         print('----')\n",
    "        \n",
    "#         rr['e4'] = total_sim2/(factor2)\n",
    "#    # print(rr['e2'])\n",
    "    \n",
    "# #     delimiter = ' '\n",
    "# #     s1 = delimiter.join(lind\n",
    "# #     s2 = delimiter.join(lin2)ddd\n",
    "# #     r1=fuzz.ratio(query, s1)\n",
    "# #     rr['e3']=r1\n",
    "# #     r1=fuzz.ratio(query, s2)\n",
    "#     #rr['e4']=None\n",
    "#     print( str(rr['e'])+'\\t'+str(rr['e2'])+'\\t'+str(rr['e3'])+'\\t'+str(rr['e4']) +'\\t'+str(rr['relevance'])) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "r1=fuzz.ratio(\"this is a test\", \"this is a test!\")\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-85a2aa7a0705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalizes vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string 1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"string 2\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute WMD as normal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word2vec_model.init_sims(replace=True) # normalizes vectors\n",
    "distance = word2vec_model.wmdistance(\"string 1\", \"string 2\")  # Compute WMD as normal.\n",
    "\n",
    "\n",
    "def word2vec_sim(row, i):\n",
    "    try:\n",
    "        sim = model.n_similarity(row[i], row[3])\n",
    "    \n",
    "        row.append(float(sim))\n",
    "        return row\n",
    "    except:\n",
    "        new_list1 = []\n",
    "        new_list2 = []\n",
    "        for w1 in row[i]:\n",
    "            if w1 in model.vocab:\n",
    "                new_list1.append(w1)\n",
    "        for w2 in row[3]:\n",
    "            if w2 in model.vocab:\n",
    "                new_list2.append(w2)\n",
    "        if len(new_list1)>0 and len(new_list2)>0:\n",
    "            sim = model.n_similarity(new_list1, new_list2)\n",
    "        else:\n",
    "            sim = 0.0\n",
    "        row.append(float(sim))\n",
    "        return row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
