{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv files to data frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv',encoding = \"latin1\")\n",
    "test_df = pd.read_csv('data/test.csv',encoding = \"latin1\")\n",
    "attributes_df = pd.read_csv('data/attributes.csv',encoding = \"latin1\")\n",
    "product_descriptions_df = pd.read_csv('data/product_descriptions.csv',encoding = \"latin1\")\n",
    "sample_submission_df = pd.read_csv('data/sample_submission.csv', encoding = \"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 int64\n",
      "product_uid        int64\n",
      "product_title     object\n",
      "search_term       object\n",
      "relevance        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_uid             int64\n",
      "product_description    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_uid             int64\n",
      "product_description    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_uid                                product_description\n",
      "0       100001  Not only do angles make joints stronger, they ...\n",
      "1       100002  BEHR Premium Textured DECKOVER is an innovativ...\n",
      "2       100003  Classic architecture meets contemporary design...\n",
      "3       100004  The Grape Solar 265-Watt Polycrystalline PV So...\n",
      "4       100005  Update your bathroom with the Delta Vero Singl...\n"
     ]
    }
   ],
   "source": [
    "print(product_descriptions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK's stopwords list and WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and tokenize the raw text by:\n",
    "    1. Convert to lower case\n",
    "    2. Remove apostrophe\n",
    "    3. Remove Punctuation\n",
    "    4. Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "   \n",
    "    # convert text to lower case\n",
    "    lowercase = str(text).lower()\n",
    "    \n",
    "    #remove 's from string\n",
    "    apoRemoved = lowercase.replace(\"'s\",\"\")\n",
    "    \n",
    "    #convert don't to dont\n",
    "    apoRemoved = apoRemoved.replace(\"'\",\"\")\n",
    "    \n",
    "    #handle other punctuations\n",
    "    transtable = str.maketrans(string.punctuation,\"                                \")\n",
    "    brokenWords = apoRemoved.translate(transtable)\n",
    "    \n",
    "    #convert string to list of words\n",
    "    listOfWords =  nltk.word_tokenize(brokenWords)\n",
    "    \n",
    "    #lemmatize text\n",
    "    lemmatizedList=[lemmatizer.lemmatize(word) for word in listOfWords]\n",
    "   \n",
    "    return lemmatizedList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'here', 'dont', 'car']\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "text = \"Dogs Here's don't cars.\"\n",
    "print(process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train_df\n",
    "def process_train_df(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \n",
    "    newdf=df\n",
    "    for i,row in newdf.iterrows():\n",
    "#         if i>5:\n",
    "#             break\n",
    "#         print(\"raw: \",text)\n",
    "        newdf.at[i,'product_title'] = process(row['product_title'])\n",
    "#         print(\"processed: \",df.iloc[i]['product_title'])\n",
    "    return newdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_uid                                      product_title  \\\n",
      "0   2       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "1   3       100001           [simpson, strong, tie, 12, gauge, angle]   \n",
      "2   9       100002  [behr, premium, textured, deckover, 1, gal, sc...   \n",
      "3  16       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "4  17       100005  [delta, vero, 1, handle, shower, only, faucet,...   \n",
      "\n",
      "          search_term  relevance  \n",
      "0       angle bracket       3.00  \n",
      "1           l bracket       2.50  \n",
      "2           deck over       3.00  \n",
      "3    rain shower head       2.33  \n",
      "4  shower only faucet       2.67  \n"
     ]
    }
   ],
   "source": [
    "processed_train_df = process_train_df(train_df)\n",
    "print(processed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process \n",
    "def process_product_descriptions_df(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \n",
    "    newdf=df\n",
    "    for i,row in newdf.iterrows():\n",
    "#         if i>5:\n",
    "#             break\n",
    "#         print(\"raw: \",text)\n",
    "        newdf.at[i,'product_description'] = process(row['product_description'])\n",
    "#         print(\"processed: \",df.iloc[i]['product_title'])\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_uid                                product_description\n",
      "0       100001  [not, only, do, angle, make, joint, stronger, ...\n",
      "1       100002  [behr, premium, textured, deckover, is, an, in...\n",
      "2       100003  [classic, architecture, meet, contemporary, de...\n",
      "3       100004  [the, grape, solar, 265, watt, polycrystalline...\n",
      "4       100005  [update, your, bathroom, with, the, delta, ver...\n"
     ]
    }
   ],
   "source": [
    "processed_product_descriptions_df = process_product_descriptions_df(product_descriptions_df)\n",
    "print(processed_product_descriptions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [not, only, do, angle, make, joint, stronger, ...\n",
      "1    [behr, premium, textured, deckover, is, an, in...\n",
      "2    [classic, architecture, meet, contemporary, de...\n",
      "3    [the, grape, solar, 265, watt, polycrystalline...\n",
      "4    [update, your, bathroom, with, the, delta, ver...\n",
      "Name: product_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_product_descriptions_df[\"product_description\"].head(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "124428\n",
      "   product_uid                                product_description         e\n",
      "0       100001  [not, only, do, angle, make, joint, stronger, ... -1.862074\n",
      "1       100002  [behr, premium, textured, deckover, is, an, in...  1.861570\n",
      "2       100003  [classic, architecture, meet, contemporary, de...  1.045406\n",
      "3       100004  [the, grape, solar, 265, watt, polycrystalline...  0.630615\n",
      "4       100005  [update, your, bathroom, with, the, delta, ver...  0.727016\n",
      "-----------------------------------------\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9583333333333334\n",
      "0.9722222222222222\n",
      "0.9722222222222222\n",
      "0.9722222222222222\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9543650793650795\n",
      "0.9543650793650795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-55ee343d2a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;31m#print(sim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mwup_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \"\"\"\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mneed_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_needs_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;31m# Note that to preserve behavior from NLTK2 we set use_min_depth=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;31m# It is possible that more accurate results could be obtained by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_needs_root\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_needs_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wordnet_corpus_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1.6'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mget_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'WordNet (\\d+\\.\\d+) Copyright'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;34m\"\"\"Return the next decoded line from the underlying stream.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mstartpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mnew_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(processed_product_descriptions_df[\"product_description\"].head(5) )\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "query = 'shower head'\n",
    "qa= query.split(\" \")\n",
    "c=0\n",
    "w1 = wordnet.synsets(\"dog\")[0]\n",
    "w2 = wordnet.synsets(\"animal\")[0]\n",
    "print(w1.wup_similarity(w2))\n",
    "sLength = len(processed_product_descriptions_df[\"product_description\"])\n",
    "print(sLength)\n",
    "e = pd.Series(np.random.randn(sLength))\n",
    "processed_product_descriptions_df = processed_product_descriptions_df.assign(e=e.values)\n",
    "print(processed_product_descriptions_df.head())\n",
    "print('-----------------------------------------')\n",
    "total_sim=0\n",
    "word_count=0\n",
    "for index,rr in processed_product_descriptions_df.iterrows():\n",
    "    #print(lin)\n",
    "    lin = rr[\"product_description\"]\n",
    "    for word1 in lin:\n",
    "        w1 = wordnet.synsets(word1)\n",
    "        for word2 in qa:\n",
    "            w2 = wordnet.synsets(word2)\n",
    "            if w1 and w2:\n",
    "                sim = w1[0].wup_similarity(w2[0])\n",
    "                #print(sim)\n",
    "                if sim!=None and sim > .8:\n",
    "                    total_sim=total_sim+sim\n",
    "                    word_count=word_count+1\n",
    "    if word_count==0:\n",
    "         rr['e']=None\n",
    "    else:\n",
    "        rr['e'] = total_sim/word_count\n",
    "    print(rr['e'])\n",
    "        \n",
    "        \n",
    "                \n",
    "#     c=c+1\n",
    "#     if c> 4:\n",
    "#         break\n",
    "# print(c)\n",
    "    #if c> 4:\n",
    "        #break\n",
    "# query = \"shower head\"\n",
    "# from lsa.search.machine import SearchMachine\n",
    "# sm = SearchMachine(latent_dimensions=150, index_backend='lsa.keeper.backends.JsonIndexBackend',\n",
    "#                    keep_index_info={'path_to_index_folder': 'index'},\n",
    "#                    db_backend='lsa.db.mysql.MySQLBackend',\n",
    "#                    db_credentials={'db_name': 'news', 'user': 'root', 'password': 'one2012gtr'},\n",
    "#                    tables_info={\n",
    "#                        'news_news': {'fields': ('title', 'text'), 'pk_field_name': 'id', 'prefix': '', 'where': 'id < 300'}\n",
    "#                    },\n",
    "#                    decimals=3,\n",
    "#                    use_tf_idf=False\n",
    "#                    )\n",
    "\n",
    "# sm.build_index()\n",
    "# res = sm.search('natural language query', with_distances=True, limit=10)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
